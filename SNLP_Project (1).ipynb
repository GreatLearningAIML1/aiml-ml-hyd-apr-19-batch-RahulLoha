{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SNLP Project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_OJgnXvd6bJ",
        "colab_type": "text"
      },
      "source": [
        "# Project Description "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3_vGo3fd6bV",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "Classification is probably the most popular task that you would deal with in real life.\n",
        "Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the information about the writer without knowing about him/her. \n",
        "\n",
        "We are going to create a classifier that predicts multiple features of the author of a given text.\n",
        "We have designed it as a Multilabel classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ-VxCUyd6ba",
        "colab_type": "text"
      },
      "source": [
        "# Dataset \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5c6_YlId6bd",
        "colab_type": "text"
      },
      "source": [
        "Blog Authorship Corpus\n",
        "Over 600,000 posts from more than 19 thousand bloggers\n",
        "\n",
        "The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n",
        "\n",
        "Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry, and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
        "\n",
        "All bloggers included in the corpus fall into one of three age groups:\n",
        "8240 \"10s\" blogs (ages 13-17),\n",
        "8086 \"20s\" blogs(ages 23-27)\n",
        "2994 \"30s\" blogs (ages 33-47)\n",
        "\n",
        "For each age group, there is an equal number of male and female bloggers.\n",
        "Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n",
        "\n",
        "Link to dataset: https://www.kaggle.com/rtatman/blog-authorship-corpus/downloads/blog-authorship-corpus.zip/2at\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zwtoe0d6bg",
        "colab_type": "text"
      },
      "source": [
        "# Approach & Steps "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjSbV4Rd6bj",
        "colab_type": "text"
      },
      "source": [
        "1.\tLoad the dataset (5 points)\n",
        "\n",
        "a.\tTip: As the dataset is large, use fewer rows. Check what is working well on your machine and decide accordingly.\n",
        "\n",
        "2.\tPreprocess rows of the “text” column (7.5 points)\n",
        "\n",
        "a.\tRemove unwanted characters\n",
        "\n",
        "b.\tConvert text to lowercase\n",
        "\n",
        "c.\tRemove unwanted spaces\n",
        "\n",
        "d.\tRemove stopwords\n",
        "\n",
        "3.\tAs we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)\n",
        "\n",
        "a.\tLabel columns to merge: “gender”, “age”, “topic”, “sign”\n",
        "b.\tAfter completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image\n",
        "\n",
        "4.\tSeparate features and labels, and split the data into training and testing (5 points)\n",
        "\n",
        "5.\tVectorize the features (5 points)\n",
        "\n",
        "a.\tCreate a Bag of Words using count vectorizer\n",
        "i.\tUse ngram_range=(1, 2)\n",
        "ii.\tVectorize training and testing features\n",
        "\n",
        "b.\tPrint the term-document matrix\n",
        "\n",
        "6.\tCreate a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference (5 points)\n",
        "\n",
        "7. Transform the labels - (7.5 points)\n",
        "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
        "a.\tConvert your train and test labels using MultiLabelBinarizer\n",
        "\n",
        "8.\t Choose a classifier - (5 points)\n",
        "\n",
        "In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression. It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large.\n",
        "\n",
        "a.\tUse a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label\n",
        "\n",
        "b.\tAs One-vs-Rest approach might not have been discussed in the sessions, we are providing you the code for that\n",
        "\n",
        "9.\tFit the classifier, make predictions and get the accuracy (5 points)\n",
        "\n",
        "a.\tPrint the following\n",
        "\n",
        "i.\tAccuracy score\n",
        "ii.\tF1 score\n",
        "iii.\tAverage precision score\n",
        "iv.\tAverage recall score\n",
        "v.\tTip: Make sure you are familiar with all of them. How would you expect the things to work for the multi-label scenario? Read about micro/macro/weighted averaging\n",
        "\n",
        "10.\t Print true label and predicted label for any five examples (7.5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obf3IiVxd6bm",
        "colab_type": "text"
      },
      "source": [
        "# 1. Load the dataset (5 points)\n",
        "\n",
        "a. Tip: As the dataset is large, use fewer rows. Check what is working well on your machine and decide accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ncdXeFgd6bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction import text \n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHJKiQlcgihs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "829d94cd-a492-44c4-a5b0-c441e215f82e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIoJy76kd6cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"blogtext.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IRMaJ9dgxwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e52f096b-dc7a-46b2-f841-608f867464ff"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55511, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiJIJtlDd6cU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "65f40c05-a5f3-4254-93e4-a52128d57d9a"
      },
      "source": [
        "data['text']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   Info has been found (+/- 100 pages,...\n",
              "1                   These are the team members:   Drewe...\n",
              "2                   In het kader van kernfusie op aarde...\n",
              "3                         testing!!!  testing!!!          \n",
              "4                     Thanks to Yahoo!'s Toolbar I can ...\n",
              "                               ...                        \n",
              "55506                                     urlLink         \n",
              "55507                                     urlLink         \n",
              "55508                                     urlLink         \n",
              "55509                                     urlLink         \n",
              "55510                                                  NaN\n",
              "Name: text, Length: 55511, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN_dfXUvd6cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = data.head(8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUq8h9Hgd6ck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "6014cc41-ad43-49d8-c01d-4d26409d5aab"
      },
      "source": [
        "new_data['label'] = new_data[new_data.columns[1:5]].apply(lambda x: (','.join(x.dropna().astype(str))),axis=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-uG2aoBd6cs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "6b21c297-560e-4ef2-e432-639c5cfa0fe9"
      },
      "source": [
        "new_data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "      <td>male,33,InvestmentBanking,Aquarius</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>2635745</td>\n",
              "      <td>female</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Pisces</td>\n",
              "      <td>01,August,2004</td>\n",
              "      <td>Today was good for me!  I had an excell...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>2635745</td>\n",
              "      <td>female</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Pisces</td>\n",
              "      <td>01,August,2004</td>\n",
              "      <td>OH MY GOODNESS!  OH MY GOODNESS!  OH MY...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>2635745</td>\n",
              "      <td>female</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Pisces</td>\n",
              "      <td>01,August,2004</td>\n",
              "      <td>Well, my day was...okay, average, long,...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>2635745</td>\n",
              "      <td>female</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Pisces</td>\n",
              "      <td>01,August,2004</td>\n",
              "      <td>Sorry for the title.  It was more like ...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>2635745</td>\n",
              "      <td>female</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Pisces</td>\n",
              "      <td>01,August,2004</td>\n",
              "      <td>READY FOR LOVE: You're sensitive but no...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ...                               label\n",
              "0     2059027  ...                 male,15,Student,Leo\n",
              "1     2059027  ...                 male,15,Student,Leo\n",
              "2     2059027  ...                 male,15,Student,Leo\n",
              "3     2059027  ...                 male,15,Student,Leo\n",
              "4     3581210  ...  male,33,InvestmentBanking,Aquarius\n",
              "...       ...  ...                                 ...\n",
              "7995  2635745  ...            female,15,Student,Pisces\n",
              "7996  2635745  ...            female,15,Student,Pisces\n",
              "7997  2635745  ...            female,15,Student,Pisces\n",
              "7998  2635745  ...            female,15,Student,Pisces\n",
              "7999  2635745  ...            female,15,Student,Pisces\n",
              "\n",
              "[8000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukm7v_Add6c2",
        "colab_type": "text"
      },
      "source": [
        "# 3. As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)\n",
        "a. Label columns to merge: “gender”, “age”, “topic”, “sign” b. After completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HX9uLKd6c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = new_data.drop(columns=['gender','age','topic','sign','id','date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIAujO7Hd6c_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "0c6af0ff-fdb3-4e14-b87b-46f0174f0451"
      },
      "source": [
        "new_data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "      <td>male,33,InvestmentBanking,Aquarius</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>Today was good for me!  I had an excell...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>OH MY GOODNESS!  OH MY GOODNESS!  OH MY...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>Well, my day was...okay, average, long,...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>Sorry for the title.  It was more like ...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>READY FOR LOVE: You're sensitive but no...</td>\n",
              "      <td>female,15,Student,Pisces</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text                               label\n",
              "0                Info has been found (+/- 100 pages,...                 male,15,Student,Leo\n",
              "1                These are the team members:   Drewe...                 male,15,Student,Leo\n",
              "2                In het kader van kernfusie op aarde...                 male,15,Student,Leo\n",
              "3                      testing!!!  testing!!!                           male,15,Student,Leo\n",
              "4                  Thanks to Yahoo!'s Toolbar I can ...  male,33,InvestmentBanking,Aquarius\n",
              "...                                                 ...                                 ...\n",
              "7995         Today was good for me!  I had an excell...            female,15,Student,Pisces\n",
              "7996         OH MY GOODNESS!  OH MY GOODNESS!  OH MY...            female,15,Student,Pisces\n",
              "7997         Well, my day was...okay, average, long,...            female,15,Student,Pisces\n",
              "7998         Sorry for the title.  It was more like ...            female,15,Student,Pisces\n",
              "7999         READY FOR LOVE: You're sensitive but no...            female,15,Student,Pisces\n",
              "\n",
              "[8000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN9RUxfFd6dG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9c59554-3fab-495f-ca83-ef2916192688"
      },
      "source": [
        "print(len(new_data.text))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDTiSYS0d6dK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f9ecd0f5-638f-4383-a8ae-ccfe08d6100e"
      },
      "source": [
        "new_data.text[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'           Info has been found (+/- 100 pages, and 4.5 MB of .pdf files) Now i have to wait untill our team leader has processed it and learns html.         '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxErkfid6dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "l1 = ('btw','zza','zzzexy','zzzzz','youuuuu')\n",
        "nlp.Defaults.stop_words.add(l1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KvnDCGMd6dV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "4b5347b2-43d8-46bd-881d-8c51a4b842aa"
      },
      "source": [
        "new_data.text"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                  Info has been found (+/- 100 pages,...\n",
              "1                  These are the team members:   Drewe...\n",
              "2                  In het kader van kernfusie op aarde...\n",
              "3                        testing!!!  testing!!!          \n",
              "4                    Thanks to Yahoo!'s Toolbar I can ...\n",
              "                              ...                        \n",
              "7995           Today was good for me!  I had an excell...\n",
              "7996           OH MY GOODNESS!  OH MY GOODNESS!  OH MY...\n",
              "7997           Well, my day was...okay, average, long,...\n",
              "7998           Sorry for the title.  It was more like ...\n",
              "7999           READY FOR LOVE: You're sensitive but no...\n",
              "Name: text, Length: 8000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uaFcif6d6db",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4cdc145-c6e3-4892-cd62-62eb532d9bd5"
      },
      "source": [
        "for i in range(len(new_data.text)):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    new_data.text[i] = new_data.text[i].lower()\n",
        "    word_tokens = tokenizer.tokenize(new_data.text[i])\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stopwords.words('english')] \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in (nlp.Defaults.stop_words or string.punctuation):\n",
        "            #if not w.isalpha():\n",
        "            filtered_sentence.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ',w ))\n",
        "   \n",
        "    new_data.text[i] = \" \".join(filtered_sentence)\n",
        "%time"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3f5b2c3915fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfiltered_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfiltered_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-3f5b2c3915fa>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfiltered_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfiltered_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp_uxyj3d6dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = new_data.text\n",
        "Y = new_data.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v7X2u5ad6do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# spliting into training and testing data sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=103)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3d5in9jd6dq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "113cd0ea-d00c-4df8-a87a-d8e1ae088050"
      },
      "source": [
        "# defining a function that accepts a vectorizer and calculates the accuracy\n",
        "def tokenize_test(vect):\n",
        "    X_train_dtm = vect.fit_transform(X_train)\n",
        "    print('Features: ', X_train_dtm.shape[1])\n",
        "    X_test_dtm = vect.transform(X_test)\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train_dtm, y_train)\n",
        "    y_train_pred = nb.predict(X_train_dtm)\n",
        "    y_pred_class = nb.predict(X_test_dtm)\n",
        "    print('Train Accuracy for NB : ', metrics.accuracy_score(y_train,y_train_pred))\n",
        "    print('Test Accuracy for NB: ', metrics.accuracy_score(y_test, y_pred_class))\n",
        "    logreg = LogisticRegression(C=1e9)\n",
        "    logreg.fit(X_train_dtm, y_train)\n",
        "    y_pred_class_LR = logreg.predict(X_test_dtm)\n",
        "#print(metrics.accuracy_score(y_test, y_pred_class))\n",
        "    y_train_LR = logreg.predict(X_train_dtm)\n",
        "    print('Train Accuracy for LR: ',metrics.accuracy_score(y_train, y_train_LR))\n",
        "    print('Test Accuracy for LR: ',metrics.accuracy_score(y_test, y_pred_class_LR))\n",
        "%time"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 4.29 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Jxc_jCd6dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "cef18502-b0e8-4890-fedc-da17f57f9fe4"
      },
      "source": [
        "# Include 1-grams and 2-grams\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  394714\n",
            "Train Accuracy for NB :  0.653\n",
            "Test Accuracy for NB:  0.383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy for LR:  0.998\n",
            "Test Accuracy for LR:  0.6075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn85qw0gd6dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0b9016a9-194b-497a-9c3b-468bf56954eb"
      },
      "source": [
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "print('Features: ', X_train_dtm.shape[1])\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_dtm, y_train)\n",
        "    #feature = nb.feature_count_\n",
        "   # print(nb.feature_count_.shape)\n",
        "y_train_pred = nb.predict(X_train_dtm)\n",
        "y_pred_class = nb.predict(X_test_dtm)\n",
        "print('Train Accuracy for NB : ', metrics.accuracy_score(y_train,y_train_pred))\n",
        "print('Test Accuracy for NB: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  394714\n",
            "Train Accuracy for NB :  0.653\n",
            "Test Accuracy for NB:  0.383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvjz3CVGd6dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "72f911e7-060f-4980-8713-3274c144d438"
      },
      "source": [
        "# features names\n",
        "feature_names = vect.get_feature_names()\n",
        "print(feature_names[50:500])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00 the', '00 then', '00 this', '00 to', '00 too', '00 week', '00 when', '00 while', '00 would', '00 yes', '000', '000 000', '000 100', '000 20', '000 acre', '000 address', '000 amish', '000 and', '000 animals', '000 anything', '000 are', '000 armored', '000 automobile', '000 big', '000 bombers', '000 cats', '000 cds', '000 checks', '000 coffee', '000 companion', '000 compared', '000 copies', '000 credits', '000 do', '000 dollar', '000 dollars', '000 english', '000 families', '000 feet', '000 fighter', '000 figure', '000 flights', '000 for', '000 he', '000 hearing', '000 households', '000 how', '000 human', '000 in', '000 iraqi', '000 it', '000 japanese', '000 jesus', '000 jobs', '000 kids', '000 lakes', '000 layers', '000 loan', '000 miles', '000 minnesota', '000 minnesotans', '000 more', '000 my', '000 new', '000 next', '000 nice', '000 oeople', '000 of', '000 on', '000 options', '000 or', '000 people', '000 per', '000 points', '000 prospects', '000 ramsey', '000 renovation', '000 see', '000 shares', '000 signatures', '000 slaughtered', '000 soldiers', '000 square', '000 strangers', '000 subject', '000 tanks', '000 times', '000 to', '000 troops', '000 usd', '000 votes', '000 was', '000 won', '000 worth', '000 xp', '000 year', '000 years', '000 you', '000058', '000058 html', '000miles', '000miles away', '001', '001 first', '002', '002 middle', '003', '003 last', '004', '004 nickname', '005', '005 gender', '006', '006 age', '007', '007 birthday', '007 game', '008', '008 height', '009', '009 hair', '00am', '00am and', '00am ride', '00am somebody', '00pm', '00pm especially', '00pm figured', '00pm the', '01', '01 08', '01 have', '01 pm', '01 who', '010', '010 eye', '0100', '0100 hrs', '0100010001101111011100100110101101111000', '010203', '010203 hehehe', '011', '011 race', '012', '012 glasses', '0128', '0128 for', '013', '013 do', '014', '014 is', '015', '015 where', '016', '016 current', '017', '017 zodiac', '018', '018 how', '019', '019 nationality', '02', '02 00', '02 18', '02 2002', '02 23', '02 and', '02 do', '02 faced', '02 first', '02 lott', '02 on', '02 pm', '02 republicans', '02 there', '02 urllink', '02 what', '02 when', '02 where', '020', '020 bad', '021', '021 piercings', '022', '022 piercings', '023', '023 tattoos', '024', '024 tattoos', '025', '025 today', '0251', '0251 lee', '026', '026 the', '027', '027 ready', '028', '028 mother', '029', '029 father', '03', '03 03', '03 10', '03 13', '03 15', '03 16', '03 am', '03 and', '03 as', '03 below', '03 chewbacca', '03 do', '03 fiesta', '03 god', '03 hate', '03 have', '03 if', '03 in', '03 pm', '03 sigh', '03 spending', '03 where', '03 world', '03 wouldn', '030', '030 step', '0300', '0300 someone', '031', '031 brother', '031120', '031120 482', '032', '032 sister', '033', '033 favorite', '034', '034 favorite', '0346', '0346 otis', '035', '035 favorite', '036', '036 worst', '037', '037 best', '038', '038 do', '039', '039 does', '04', '04 04', '04 10', '04 16', '04 added', '04 apocalypse', '04 asses', '04 because', '04 better', '04 climate_change', '04 compassionate', '04 email', '04 four', '04 get', '04 have', '04 he', '04 in', '04 is', '04 it', '04 leave', '04 less', '04 making', '04 or', '04 pm', '04 putting', '04 thanks', '04 the', '04 this', '04 ticket', '04 was', '04 we', '04 yes', '04 you', '040', '040 do', '0400', '0400 starin', '041', '041 what', '042', '042 what', '043', '043 are', '0431', '0431 travsd', '044', '044 did', '045', '045 current', '046', '046 favorite', '047', '047 least', '048', '048 favorite', '049', '049 least', '05', '05 04', '05 07', '05 13', '05 22', '05 25', '05 31', '05 and', '05 drugwar', '05 have', '05 jul', '05 makes', '05 on', '05 pm', '05 quickly', '05 short', '050', '050 favorite', '0505', '0505 from', '051', '051 least', '052', '052 do', '053', '053 play', '054', '054 do', '055', '055 are', '056', '056 favorite', '057', '057 favorite', '058', '058 least', '059', '059 least', '05pm', '05pm and', '06', '06 04', '06 07', '06 09', '06 10', '06 11', '06 12', '06 19', '06 do', '06 has', '06 pm', '06 pst', '060', '060 most', '0600', '0600 please', '0600 this', '061', '061 number', '062', '062 clothing', '063', '063 shoes', '064', '064 saying', '065', '065 tv', '066', '066 sport', '067', '067 vegetable', '068', '068 fruit', '069', '069 movie', '06th', '06th 1987', '07', '07 20', '07 24', '07 29', '07 41', '07 46', '07 47', '07 always', '07 climate', '07 have', '07 pm', '07 wow', '070', '070 magazine', '0700', '0700 at', '071', '071 actor', '072', '072 actress', '073', '073 candy', '074', '074 gum', '075', '075 scent', '076', '076 candy', '077', '077 ice', '078', '078 color', '079', '079 season', '08', '08 29', '08 60ii', '08 am', '08 ever', '08 is', '080', '080 holiday', '0800', '0800 you', '081', '081 band', '082', '082 singer', '083', '083 group', '084', '084 rapper', '085', '085 type', '086', '086 thing', '087', '087 place', '088', '088 radio', '089', '089 tv', '09', '09 11', '09 ever']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0jrszyQd6d3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "060cd352-d4a9-4b7c-803e-8695cc8424c4"
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbynvraJd6d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqvzjYTd6d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = y_train.apply(lambda x : pd.value_counts(x.split(\",\"))).sum(axis = 0).to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHdyXN3nd6d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "d035da2e-9fdf-4323-dca3-65f0c81d79db"
      },
      "source": [
        "ds"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'13': 8.0,\n",
              " '14': 131.0,\n",
              " '15': 289.0,\n",
              " '16': 57.0,\n",
              " '17': 707.0,\n",
              " '23': 102.0,\n",
              " '24': 281.0,\n",
              " '25': 201.0,\n",
              " '26': 87.0,\n",
              " '27': 526.0,\n",
              " '33': 82.0,\n",
              " '34': 411.0,\n",
              " '35': 1721.0,\n",
              " '36': 1246.0,\n",
              " '37': 14.0,\n",
              " '38': 32.0,\n",
              " '39': 61.0,\n",
              " '41': 13.0,\n",
              " '42': 12.0,\n",
              " '44': 2.0,\n",
              " '45': 11.0,\n",
              " '46': 6.0,\n",
              " 'Aquarius': 270.0,\n",
              " 'Aries': 3070.0,\n",
              " 'Arts': 24.0,\n",
              " 'Automotive': 12.0,\n",
              " 'Banking': 14.0,\n",
              " 'BusinessServices': 63.0,\n",
              " 'Cancer': 166.0,\n",
              " 'Capricorn': 73.0,\n",
              " 'Communications-Media': 51.0,\n",
              " 'Consulting': 16.0,\n",
              " 'Education': 92.0,\n",
              " 'Engineering': 86.0,\n",
              " 'Fashion': 1184.0,\n",
              " 'Gemini': 64.0,\n",
              " 'Internet': 66.0,\n",
              " 'InvestmentBanking': 57.0,\n",
              " 'Law': 2.0,\n",
              " 'Leo': 160.0,\n",
              " 'Libra': 321.0,\n",
              " 'Museums-Libraries': 2.0,\n",
              " 'Non-Profit': 30.0,\n",
              " 'Pisces': 75.0,\n",
              " 'Religion': 7.0,\n",
              " 'Sagittarius': 562.0,\n",
              " 'Science': 27.0,\n",
              " 'Scorpio': 669.0,\n",
              " 'Sports-Recreation': 63.0,\n",
              " 'Student': 461.0,\n",
              " 'Taurus': 539.0,\n",
              " 'Technology': 1752.0,\n",
              " 'Virgo': 31.0,\n",
              " 'female': 2328.0,\n",
              " 'indUnk': 1991.0,\n",
              " 'male': 3672.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD7t-PH4d6eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvDPfukdd6eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlb = MultiLabelBinarizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM9ftOTad6eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_mlb = mlb.fit_transform(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNJ24Cbgd6eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_mlb = mlb.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQV_LHAWd6eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj2SfQnWd6eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = LogisticRegression(solver = 'lbfgs',random_state= 101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLIZdunNd6eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "b3031d4b-46c9-4188-de0e-44e9f66be8af"
      },
      "source": [
        "LR"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=101, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5gIvGrpd6eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = OneVsRestClassifier(LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHF11P2nd6eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = vect.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Po4FjdNd6eV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "d45877bc-6d19-4679-ed50-28293d8d5e85"
      },
      "source": [
        "y_train_mlb"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_63SpedFd6eX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06eafebc-dc15-4e5a-86c5-d8f23d819717"
      },
      "source": [
        "clf.fit(X_train_dtm,y_train_mlb)\n",
        "%time"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 0 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 27 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 31 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 37 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 38 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
            "Wall time: 7.63 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCEaxmOfd6eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_clf = clf.predict(X_test_dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA9iz42Nd6ec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ebb08df-d41e-4b1a-969e-dd03ac0a0872"
      },
      "source": [
        "print(metrics.accuracy_score(y_test_mlb,y_pred_clf))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQzCt9O3d6ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ecYrdhPd6ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf018062-1d83-43e4-b7cb-52b2e2f34c53"
      },
      "source": [
        "print(classification_report(y_test_mlb, y_pred_clf))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2000\n",
            "           1       0.69      0.22      0.33        41\n",
            "           2       0.87      0.55      0.68       365\n",
            "           3       0.71      0.36      0.48       398\n",
            "           4       0.85      0.86      0.85      1275\n",
            "           5       0.86      0.49      0.63       273\n",
            "           6       0.77      0.63      0.69       761\n",
            "           7       0.91      0.54      0.68       499\n",
            "           8       0.81      0.41      0.54       377\n",
            "           9       0.67      0.14      0.24        14\n",
            "          10       0.50      0.06      0.10        18\n",
            "          11       0.84      0.84      0.84      1156\n",
            "          12       1.00      0.21      0.34        39\n",
            "          13       0.76      0.25      0.38        99\n",
            "          14       0.77      0.16      0.27        62\n",
            "          15       0.95      0.58      0.72       432\n",
            "          16       0.00      0.00      0.00        24\n",
            "          17       0.88      0.17      0.29        40\n",
            "          18       0.68      0.24      0.35       152\n",
            "          19       0.33      0.10      0.15        10\n",
            "          20       0.00      0.00      0.00        17\n",
            "          21       0.40      0.04      0.08        45\n",
            "          22       0.88      0.44      0.58        16\n",
            "          23       0.81      0.54      0.65       539\n",
            "          24       0.79      0.77      0.78       768\n",
            "          25       0.78      0.51      0.62       625\n",
            "          26       0.00      0.00      0.00        10\n",
            "          27       1.00      1.00      1.00      2000\n",
            "          28       0.73      0.23      0.35       104\n",
            "          29       0.79      0.78      0.78       964\n",
            "          30       0.83      0.66      0.73       818\n",
            "          31       1.00      1.00      1.00      2000\n",
            "          32       0.81      0.62      0.70       750\n",
            "          33       0.80      0.69      0.74       854\n",
            "          34       0.85      0.81      0.83      1030\n",
            "          35       0.98      1.00      0.99      1949\n",
            "          36       0.78      0.53      0.63       640\n",
            "          37       1.00      1.00      1.00      2000\n",
            "          38       1.00      1.00      1.00      2000\n",
            "          39       1.00      1.00      1.00      1994\n",
            "          40       0.84      0.89      0.86      1336\n",
            "          41       0.72      0.32      0.44       201\n",
            "          42       0.68      0.16      0.26        81\n",
            "          43       0.96      0.99      0.97      1901\n",
            "          44       0.89      0.95      0.92      1577\n",
            "          45       0.80      0.46      0.58       442\n",
            "          46       0.77      0.52      0.62       582\n",
            "          47       1.00      0.23      0.38        39\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.80      0.65      0.72       598\n",
            "\n",
            "   micro avg       0.91      0.83      0.87     33916\n",
            "   macro avg       0.75      0.51      0.58     33916\n",
            "weighted avg       0.90      0.83      0.85     33916\n",
            " samples avg       0.91      0.83      0.86     33916\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smPybidcd6em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65873839-2ff2-46ba-f5cf-3e2263d8f1f5"
      },
      "source": [
        "metrics.average_precision_score(y_test_mlb, y_pred_clf,average='micro')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8121708958508178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSx-wdtSjQW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf4e6659-20e8-4545-a2a9-1a9843b83c80"
      },
      "source": [
        "metrics.recall_score(y_test_mlb, y_pred_clf, labels=None, pos_label=1, average='micro', sample_weight=None)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8263946220073122"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USdy3gxud6ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "868a82f0-4f55-416c-8e29-651bce94430c"
      },
      "source": [
        "y_pred_clf[10:15]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05in2YBd6e0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "1f7a9074-d338-4fe2-809b-49bbaf5e40e8"
      },
      "source": [
        "y_test_mlb[10:15]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
              "        1, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
              "        1, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLFeLC7UjaXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}