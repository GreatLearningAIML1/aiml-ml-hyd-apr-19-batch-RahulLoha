{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Eager Execution if you are using tensorflow 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnSsH8sNOB6F",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  \n",
       "3  2006300.0  \n",
       "4  1408600.0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['date','symbol'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:1000]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Float64 to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      float64\n",
       "close     float64\n",
       "low       float64\n",
       "high      float64\n",
       "volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['open'] = data['open'].astype('float32')\n",
    "data['close'] = data['close'].astype('float32')\n",
    "data['low'] = data['low'].astype('float32')\n",
    "data['high'] = data['high'].astype('float32')\n",
    "data['volume'] = data['volume'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(['close'], axis =1)\n",
    "y = data[\"close\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Train and Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer()\n",
    "X_train = transformer.fit_transform(X_train)\n",
    "X_test = transformer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the graph in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "w = tf.zeros(shape=(4,1))\n",
    "b = tf.zeros(shape=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "def prediction(x, w, b):\n",
    "    \n",
    "    xw_matmul = tf.matmul(x, w)\n",
    "    y = tf.add(xw_matmul, b)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "def loss(y_actual, y_predicted):\n",
    "    \n",
    "    diff = y_actual - y_predicted\n",
    "    sqr = tf.square(diff)\n",
    "    avg = tf.reduce_mean(sqr)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5.GradientDescent Optimizer to minimize Loss [GradientDescentOptimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "def train(x, y_actual, w, b, learning_rate=0.01):\n",
    "    \n",
    "    #Record mathematical operations on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        t.watch([w,b])\n",
    "        \n",
    "        current_prediction = prediction(x, w, b)\n",
    "        current_loss = loss(y_actual, current_prediction)\n",
    "    \n",
    "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    \n",
    "    #Update Weights and Bias\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Execute the Graph for 100 epochs and observe the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVvgj7eQOB7f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1784944e-06, 1.1751102e-06, 1.2123350e-06, 1.0000000e+00],\n",
       "       [3.2204923e-06, 3.0719973e-06, 3.2326736e-06, 1.0000000e+00],\n",
       "       [1.2140832e-05, 1.2107108e-05, 1.2292594e-05, 1.0000000e+00],\n",
       "       ...,\n",
       "       [3.9534378e-05, 3.7634643e-05, 3.9534378e-05, 1.0000000e+00],\n",
       "       [2.3261399e-04, 2.3111279e-04, 2.3355226e-04, 1.0000000e+00],\n",
       "       [6.6367811e-06, 6.6367811e-06, 6.8007871e-06, 1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9smwOW-1OB7k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss on iteration 0 7783.29\n",
      "Current Loss on iteration 1 7447.384\n",
      "Current Loss on iteration 2 7137.816\n",
      "Current Loss on iteration 3 6852.516\n",
      "Current Loss on iteration 4 6589.5815\n",
      "Current Loss on iteration 5 6347.2676\n",
      "Current Loss on iteration 6 6123.947\n",
      "Current Loss on iteration 7 5918.1377\n",
      "Current Loss on iteration 8 5728.46\n",
      "Current Loss on iteration 9 5553.6567\n",
      "Current Loss on iteration 10 5392.554\n",
      "Current Loss on iteration 11 5244.0845\n",
      "Current Loss on iteration 12 5107.253\n",
      "Current Loss on iteration 13 4981.152\n",
      "Current Loss on iteration 14 4864.935\n",
      "Current Loss on iteration 15 4757.83\n",
      "Current Loss on iteration 16 4659.121\n",
      "Current Loss on iteration 17 4568.1523\n",
      "Current Loss on iteration 18 4484.3164\n",
      "Current Loss on iteration 19 4407.052\n",
      "Current Loss on iteration 20 4335.844\n",
      "Current Loss on iteration 21 4270.2207\n",
      "Current Loss on iteration 22 4209.7397\n",
      "Current Loss on iteration 23 4154.004\n",
      "Current Loss on iteration 24 4102.634\n",
      "Current Loss on iteration 25 4055.2927\n",
      "Current Loss on iteration 26 4011.664\n",
      "Current Loss on iteration 27 3971.4553\n",
      "Current Loss on iteration 28 3934.4011\n",
      "Current Loss on iteration 29 3900.2483\n",
      "Current Loss on iteration 30 3868.777\n",
      "Current Loss on iteration 31 3839.7695\n",
      "Current Loss on iteration 32 3813.039\n",
      "Current Loss on iteration 33 3788.4006\n",
      "Current Loss on iteration 34 3765.6973\n",
      "Current Loss on iteration 35 3744.7708\n",
      "Current Loss on iteration 36 3725.488\n",
      "Current Loss on iteration 37 3707.7148\n",
      "Current Loss on iteration 38 3691.3367\n",
      "Current Loss on iteration 39 3676.2422\n",
      "Current Loss on iteration 40 3662.3306\n",
      "Current Loss on iteration 41 3649.509\n",
      "Current Loss on iteration 42 3637.6938\n",
      "Current Loss on iteration 43 3626.8047\n",
      "Current Loss on iteration 44 3616.7693\n",
      "Current Loss on iteration 45 3607.5212\n",
      "Current Loss on iteration 46 3598.997\n",
      "Current Loss on iteration 47 3591.1392\n",
      "Current Loss on iteration 48 3583.9026\n",
      "Current Loss on iteration 49 3577.2295\n",
      "Current Loss on iteration 50 3571.0808\n",
      "Current Loss on iteration 51 3565.4136\n",
      "Current Loss on iteration 52 3560.1924\n",
      "Current Loss on iteration 53 3555.379\n",
      "Current Loss on iteration 54 3550.942\n",
      "Current Loss on iteration 55 3546.8552\n",
      "Current Loss on iteration 56 3543.088\n",
      "Current Loss on iteration 57 3539.6152\n",
      "Current Loss on iteration 58 3536.4148\n",
      "Current Loss on iteration 59 3533.4663\n",
      "Current Loss on iteration 60 3530.749\n",
      "Current Loss on iteration 61 3528.2444\n",
      "Current Loss on iteration 62 3525.9346\n",
      "Current Loss on iteration 63 3523.81\n",
      "Current Loss on iteration 64 3521.8484\n",
      "Current Loss on iteration 65 3520.04\n",
      "Current Loss on iteration 66 3518.3757\n",
      "Current Loss on iteration 67 3516.8403\n",
      "Current Loss on iteration 68 3515.4258\n",
      "Current Loss on iteration 69 3514.1226\n",
      "Current Loss on iteration 70 3512.9211\n",
      "Current Loss on iteration 71 3511.8147\n",
      "Current Loss on iteration 72 3510.793\n",
      "Current Loss on iteration 73 3509.8533\n",
      "Current Loss on iteration 74 3508.9863\n",
      "Current Loss on iteration 75 3508.189\n",
      "Current Loss on iteration 76 3507.4531\n",
      "Current Loss on iteration 77 3506.7742\n",
      "Current Loss on iteration 78 3506.1484\n",
      "Current Loss on iteration 79 3505.5713\n",
      "Current Loss on iteration 80 3505.0413\n",
      "Current Loss on iteration 81 3504.5522\n",
      "Current Loss on iteration 82 3504.1016\n",
      "Current Loss on iteration 83 3503.6855\n",
      "Current Loss on iteration 84 3503.3027\n",
      "Current Loss on iteration 85 3502.9485\n",
      "Current Loss on iteration 86 3502.6267\n",
      "Current Loss on iteration 87 3502.3247\n",
      "Current Loss on iteration 88 3502.05\n",
      "Current Loss on iteration 89 3501.7942\n",
      "Current Loss on iteration 90 3501.5593\n",
      "Current Loss on iteration 91 3501.3433\n",
      "Current Loss on iteration 92 3501.146\n",
      "Current Loss on iteration 93 3500.9617\n",
      "Current Loss on iteration 94 3500.7903\n",
      "Current Loss on iteration 95 3500.6333\n",
      "Current Loss on iteration 96 3500.4917\n",
      "Current Loss on iteration 97 3500.357\n",
      "Current Loss on iteration 98 3500.2363\n",
      "Current Loss on iteration 99 3500.124\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    w, b = train(X_train, np.array(y_train), w, b)\n",
    "    print('Current Loss on iteration', i, loss(np.array(y_train), prediction(X_train, w, b)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JuLI6bSOB7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1)])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(1)])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [[2.7464107e-03]\n",
      " [2.7223732e-03]\n",
      " [2.7687277e-03]\n",
      " [3.3516525e+01]]\n",
      "Bias:\n",
      " [33.51653]\n"
     ]
    }
   ],
   "source": [
    "print('Weights:\\n', w.numpy())\n",
    "print('Bias:\\n',b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhDtOv5UOB7x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "### Linear Classification using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GoNTWXAOB8C",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
    "#### Use Mean square error as loss function and sgd as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpeL5rCTOB8D"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential Graph (model)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(4,)))\n",
    "\n",
    "#Compile the model - add Loss and Gradient Descent optimizer\n",
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt-HYFMEOB8G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66JGJt7GOB8H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "670/670 [==============================] - 0s 183us/sample - loss: 5828.5529\n",
      "Epoch 2/100\n",
      "670/670 [==============================] - 0s 39us/sample - loss: 3923.9783\n",
      "Epoch 3/100\n",
      "670/670 [==============================] - 0s 40us/sample - loss: 3579.5837\n",
      "Epoch 4/100\n",
      "670/670 [==============================] - 0s 48us/sample - loss: 3517.8824\n",
      "Epoch 5/100\n",
      "670/670 [==============================] - 0s 51us/sample - loss: 3505.4499\n",
      "Epoch 6/100\n",
      "670/670 [==============================] - 0s 51us/sample - loss: 3502.9023\n",
      "Epoch 7/100\n",
      "670/670 [==============================] - 0s 58us/sample - loss: 3500.9678\n",
      "Epoch 8/100\n",
      "670/670 [==============================] - 0s 60us/sample - loss: 3501.3873\n",
      "Epoch 9/100\n",
      "670/670 [==============================] - 0s 63us/sample - loss: 3500.6558\n",
      "Epoch 10/100\n",
      "670/670 [==============================] - 0s 60us/sample - loss: 3502.3676\n",
      "Epoch 11/100\n",
      "670/670 [==============================] - 0s 63us/sample - loss: 3507.5586\n",
      "Epoch 12/100\n",
      "670/670 [==============================] - 0s 55us/sample - loss: 3502.8213\n",
      "Epoch 13/100\n",
      "670/670 [==============================] - 0s 46us/sample - loss: 3501.6884\n",
      "Epoch 14/100\n",
      "670/670 [==============================] - 0s 61us/sample - loss: 3503.1791\n",
      "Epoch 15/100\n",
      "670/670 [==============================] - 0s 63us/sample - loss: 3504.4758\n",
      "Epoch 16/100\n",
      "670/670 [==============================] - 0s 66us/sample - loss: 3502.3709\n",
      "Epoch 17/100\n",
      "670/670 [==============================] - 0s 52us/sample - loss: 3502.2671\n",
      "Epoch 18/100\n",
      "670/670 [==============================] - 0s 57us/sample - loss: 3501.6633\n",
      "Epoch 19/100\n",
      "670/670 [==============================] - 0s 60us/sample - loss: 3503.2589\n",
      "Epoch 20/100\n",
      "670/670 [==============================] - 0s 60us/sample - loss: 3502.2323\n",
      "Epoch 21/100\n",
      "670/670 [==============================] - 0s 54us/sample - loss: 3501.1129\n",
      "Epoch 22/100\n",
      "670/670 [==============================] - 0s 61us/sample - loss: 3501.9373\n",
      "Epoch 23/100\n",
      "670/670 [==============================] - 0s 52us/sample - loss: 3501.3761\n",
      "Epoch 24/100\n",
      "670/670 [==============================] - 0s 49us/sample - loss: 3502.2682\n",
      "Epoch 25/100\n",
      "670/670 [==============================] - 0s 54us/sample - loss: 3502.8185\n",
      "Epoch 26/100\n",
      "670/670 [==============================] - 0s 51us/sample - loss: 3502.4268\n",
      "Epoch 27/100\n",
      "670/670 [==============================] - 0s 52us/sample - loss: 3502.8360\n",
      "Epoch 28/100\n",
      "670/670 [==============================] - 0s 65us/sample - loss: 3502.3349\n",
      "Epoch 29/100\n",
      "670/670 [==============================] - 0s 52us/sample - loss: 3502.3200\n",
      "Epoch 30/100\n",
      "670/670 [==============================] - 0s 52us/sample - loss: 3504.4608\n",
      "Epoch 31/100\n",
      "670/670 [==============================] - 0s 65us/sample - loss: 3502.9217\n",
      "Epoch 32/100\n",
      "670/670 [==============================] - 0s 54us/sample - loss: 3503.7057\n",
      "Epoch 33/100\n",
      "670/670 [==============================] - 0s 48us/sample - loss: 3502.3286\n",
      "Epoch 34/100\n",
      "670/670 [==============================] - 0s 48us/sample - loss: 3500.0536\n",
      "Epoch 35/100\n",
      "670/670 [==============================] - 0s 55us/sample - loss: 3504.6452\n",
      "Epoch 36/100\n",
      "670/670 [==============================] - 0s 49us/sample - loss: 3501.8027\n",
      "Epoch 37/100\n",
      "670/670 [==============================] - 0s 51us/sample - loss: 3500.7442\n",
      "Epoch 38/100\n",
      "670/670 [==============================] - 0s 46us/sample - loss: 3502.1018\n",
      "Epoch 39/100\n",
      "670/670 [==============================] - 0s 58us/sample - loss: 3501.1106\n",
      "Epoch 40/100\n",
      "670/670 [==============================] - 0s 57us/sample - loss: 3502.0742\n",
      "Epoch 41/100\n",
      "670/670 [==============================] - 0s 61us/sample - loss: 3505.2815\n",
      "Epoch 42/100\n",
      "670/670 [==============================] - 0s 67us/sample - loss: 3503.7404\n",
      "Epoch 43/100\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 3501.9058\n",
      "Epoch 44/100\n",
      "670/670 [==============================] - 0s 71us/sample - loss: 3505.3482\n",
      "Epoch 45/100\n",
      "670/670 [==============================] - 0s 57us/sample - loss: 3501.6949\n",
      "Epoch 46/100\n",
      "670/670 [==============================] - 0s 73us/sample - loss: 3501.3567\n",
      "Epoch 47/100\n",
      "670/670 [==============================] - 0s 58us/sample - loss: 3503.3982\n",
      "Epoch 48/100\n",
      "670/670 [==============================] - 0s 71us/sample - loss: 3503.9206\n",
      "Epoch 49/100\n",
      "670/670 [==============================] - 0s 65us/sample - loss: 3503.3868\n",
      "Epoch 50/100\n",
      "670/670 [==============================] - 0s 64us/sample - loss: 3504.2057\n",
      "Epoch 51/100\n",
      "670/670 [==============================] - 0s 67us/sample - loss: 3502.1646\n",
      "Epoch 52/100\n",
      "670/670 [==============================] - 0s 57us/sample - loss: 3502.0281\n",
      "Epoch 53/100\n",
      "670/670 [==============================] - 0s 73us/sample - loss: 3500.9686\n",
      "Epoch 54/100\n",
      "670/670 [==============================] - 0s 85us/sample - loss: 3504.5655\n",
      "Epoch 55/100\n",
      "670/670 [==============================] - 0s 64us/sample - loss: 3503.0740\n",
      "Epoch 56/100\n",
      "670/670 [==============================] - 0s 60us/sample - loss: 3501.4710\n",
      "Epoch 57/100\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 3503.8555\n",
      "Epoch 58/100\n",
      "670/670 [==============================] - 0s 79us/sample - loss: 3502.4375\n",
      "Epoch 59/100\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 3501.1096\n",
      "Epoch 60/100\n",
      "670/670 [==============================] - 0s 83us/sample - loss: 3502.1750\n",
      "Epoch 61/100\n",
      "670/670 [==============================] - 0s 92us/sample - loss: 3502.7772s - loss: 3579.14\n",
      "Epoch 62/100\n",
      "670/670 [==============================] - 0s 73us/sample - loss: 3503.0358\n",
      "Epoch 63/100\n",
      "670/670 [==============================] - 0s 74us/sample - loss: 3502.0782\n",
      "Epoch 64/100\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 3502.8496\n",
      "Epoch 65/100\n",
      "670/670 [==============================] - 0s 83us/sample - loss: 3502.2222\n",
      "Epoch 66/100\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 3502.2453\n",
      "Epoch 67/100\n",
      "670/670 [==============================] - 0s 76us/sample - loss: 3503.3856\n",
      "Epoch 68/100\n",
      "670/670 [==============================] - 0s 76us/sample - loss: 3503.5450\n",
      "Epoch 69/100\n",
      "670/670 [==============================] - 0s 64us/sample - loss: 3505.2886\n",
      "Epoch 70/100\n",
      "670/670 [==============================] - 0s 58us/sample - loss: 3501.2520\n",
      "Epoch 71/100\n",
      "670/670 [==============================] - 0s 80us/sample - loss: 3502.0283\n",
      "Epoch 72/100\n",
      "670/670 [==============================] - 0s 82us/sample - loss: 3503.1029\n",
      "Epoch 73/100\n",
      "670/670 [==============================] - 0s 80us/sample - loss: 3502.6183\n",
      "Epoch 74/100\n",
      "670/670 [==============================] - 0s 71us/sample - loss: 3499.7362\n",
      "Epoch 75/100\n",
      "670/670 [==============================] - 0s 79us/sample - loss: 3504.9136\n",
      "Epoch 76/100\n",
      "670/670 [==============================] - 0s 77us/sample - loss: 3504.7107\n",
      "Epoch 77/100\n",
      "670/670 [==============================] - 0s 73us/sample - loss: 3504.8020\n",
      "Epoch 78/100\n",
      "670/670 [==============================] - 0s 79us/sample - loss: 3504.7162\n",
      "Epoch 79/100\n",
      "670/670 [==============================] - 0s 76us/sample - loss: 3503.8261\n",
      "Epoch 80/100\n",
      "670/670 [==============================] - 0s 83us/sample - loss: 3501.1265\n",
      "Epoch 81/100\n",
      "670/670 [==============================] - 0s 97us/sample - loss: 3503.8302\n",
      "Epoch 82/100\n",
      "670/670 [==============================] - 0s 79us/sample - loss: 3503.3249\n",
      "Epoch 83/100\n",
      "670/670 [==============================] - 0s 70us/sample - loss: 3502.2122\n",
      "Epoch 84/100\n",
      "670/670 [==============================] - 0s 70us/sample - loss: 3502.9805\n",
      "Epoch 85/100\n",
      "670/670 [==============================] - 0s 70us/sample - loss: 3503.5676\n",
      "Epoch 86/100\n",
      "670/670 [==============================] - 0s 67us/sample - loss: 3503.6270\n",
      "Epoch 87/100\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 3503.1177\n",
      "Epoch 88/100\n",
      "670/670 [==============================] - 0s 68us/sample - loss: 3501.4988\n",
      "Epoch 89/100\n",
      "670/670 [==============================] - 0s 67us/sample - loss: 3502.1370\n",
      "Epoch 90/100\n",
      "670/670 [==============================] - 0s 98us/sample - loss: 3502.1608\n",
      "Epoch 91/100\n",
      "670/670 [==============================] - 0s 110us/sample - loss: 3501.8039\n",
      "Epoch 92/100\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 3501.5754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "670/670 [==============================] - 0s 113us/sample - loss: 3502.8810\n",
      "Epoch 94/100\n",
      "670/670 [==============================] - 0s 106us/sample - loss: 3503.8878\n",
      "Epoch 95/100\n",
      "670/670 [==============================] - 0s 115us/sample - loss: 3505.7923\n",
      "Epoch 96/100\n",
      "670/670 [==============================] - 0s 101us/sample - loss: 3501.1908\n",
      "Epoch 97/100\n",
      "670/670 [==============================] - 0s 103us/sample - loss: 3501.0615\n",
      "Epoch 98/100\n",
      "670/670 [==============================] - 0s 107us/sample - loss: 3502.1628\n",
      "Epoch 99/100\n",
      "670/670 [==============================] - 0s 103us/sample - loss: 3503.4499\n",
      "Epoch 100/100\n",
      "670/670 [==============================] - 0s 104us/sample - loss: 3501.4544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1420046bda0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris = pd.read_csv(\"Iris.csv\")\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_iris.drop(['Id','Species'], axis =1)\n",
    "y = df_iris[\"Species\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()\n",
    "df_iris[\"Species\"] = le.fit_transform(df_iris[\"Species\"])\n",
    "y=df_iris[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= pd.get_dummies(y).values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "4              5.0           3.6            1.4           0.2\n",
       "123            6.3           2.7            4.9           1.8\n",
       "90             5.5           2.6            4.4           1.2\n",
       "3              4.6           3.1            1.5           0.2\n",
       "17             5.1           3.5            1.4           0.3\n",
       "41             4.5           2.3            1.3           0.3\n",
       "9              4.9           3.1            1.5           0.1\n",
       "75             6.6           3.0            4.4           1.4\n",
       "111            6.4           2.7            5.3           1.9\n",
       "80             5.5           2.4            3.8           1.1\n",
       "62             6.0           2.2            4.0           1.0\n",
       "79             5.7           2.6            3.5           1.0\n",
       "133            6.3           2.8            5.1           1.5\n",
       "31             5.4           3.4            1.5           0.4\n",
       "116            6.5           3.0            5.5           1.8\n",
       "50             7.0           3.2            4.7           1.4\n",
       "95             5.7           3.0            4.2           1.2\n",
       "82             5.8           2.7            3.9           1.2\n",
       "81             5.5           2.4            3.7           1.0\n",
       "1              4.9           3.0            1.4           0.2\n",
       "146            6.3           2.5            5.0           1.9\n",
       "89             5.5           2.5            4.0           1.3\n",
       "2              4.7           3.2            1.3           0.2\n",
       "32             5.2           4.1            1.5           0.1\n",
       "92             5.8           2.6            4.0           1.2\n",
       "14             5.8           4.0            1.2           0.2\n",
       "101            5.8           2.7            5.1           1.9\n",
       "48             5.3           3.7            1.5           0.2\n",
       "104            6.5           3.0            5.8           2.2\n",
       "115            6.4           3.2            5.3           2.3\n",
       "..             ...           ...            ...           ...\n",
       "138            6.0           3.0            4.8           1.8\n",
       "60             5.0           2.0            3.5           1.0\n",
       "141            6.9           3.1            5.1           2.3\n",
       "65             6.7           3.1            4.4           1.4\n",
       "55             5.7           2.8            4.5           1.3\n",
       "102            7.1           3.0            5.9           2.1\n",
       "91             6.1           3.0            4.6           1.4\n",
       "19             5.1           3.8            1.5           0.3\n",
       "97             6.2           2.9            4.3           1.3\n",
       "145            6.7           3.0            5.2           2.3\n",
       "10             5.4           3.7            1.5           0.2\n",
       "30             4.8           3.1            1.6           0.2\n",
       "135            7.7           3.0            6.1           2.3\n",
       "130            7.4           2.8            6.1           1.9\n",
       "140            6.7           3.1            5.6           2.4\n",
       "58             6.6           2.9            4.6           1.3\n",
       "39             5.1           3.4            1.5           0.2\n",
       "68             6.2           2.2            4.5           1.5\n",
       "49             5.0           3.3            1.4           0.2\n",
       "72             6.3           2.5            4.9           1.5\n",
       "63             6.1           2.9            4.7           1.4\n",
       "122            7.7           2.8            6.7           2.0\n",
       "85             6.0           3.4            4.5           1.6\n",
       "59             5.2           2.7            3.9           1.4\n",
       "106            4.9           2.5            4.5           1.7\n",
       "23             5.1           3.3            1.7           0.5\n",
       "18             5.7           3.8            1.7           0.3\n",
       "21             5.1           3.7            1.5           0.4\n",
       "35             5.0           3.2            1.2           0.2\n",
       "56             6.3           3.3            4.7           1.6\n",
       "\n",
       "[105 rows x 4 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "47             4.6           3.2            1.4           0.2\n",
       "109            7.2           3.6            6.1           2.5\n",
       "61             5.9           3.0            4.2           1.5\n",
       "120            6.9           3.2            5.7           2.3\n",
       "124            6.7           3.3            5.7           2.1\n",
       "46             5.1           3.8            1.6           0.2\n",
       "51             6.4           3.2            4.5           1.5\n",
       "103            6.3           2.9            5.6           1.8\n",
       "37             4.9           3.1            1.5           0.1\n",
       "149            5.9           3.0            5.1           1.8\n",
       "45             4.8           3.0            1.4           0.3\n",
       "121            5.6           2.8            4.9           2.0\n",
       "147            6.5           3.0            5.2           2.0\n",
       "142            5.8           2.7            5.1           1.9\n",
       "118            7.7           2.6            6.9           2.3\n",
       "29             4.7           3.2            1.6           0.2\n",
       "93             5.0           2.3            3.3           1.0\n",
       "43             5.0           3.5            1.6           0.6\n",
       "99             5.7           2.8            4.1           1.3\n",
       "128            6.4           2.8            5.6           2.1\n",
       "12             4.8           3.0            1.4           0.1\n",
       "96             5.7           2.9            4.2           1.3\n",
       "127            6.1           3.0            4.9           1.8\n",
       "27             5.2           3.5            1.5           0.2\n",
       "74             6.4           2.9            4.3           1.3\n",
       "73             6.1           2.8            4.7           1.2\n",
       "131            7.9           3.8            6.4           2.0\n",
       "83             6.0           2.7            5.1           1.6\n",
       "8              4.4           2.9            1.4           0.2\n",
       "86             6.7           3.1            4.7           1.5\n",
       "69             5.6           2.5            3.9           1.1\n",
       "107            7.3           2.9            6.3           1.8\n",
       "144            6.7           3.3            5.7           2.5\n",
       "108            6.7           2.5            5.8           1.8\n",
       "148            6.2           3.4            5.4           2.3\n",
       "24             4.8           3.4            1.9           0.2\n",
       "53             5.5           2.3            4.0           1.3\n",
       "136            6.3           3.4            5.6           2.4\n",
       "100            6.3           3.3            6.0           2.5\n",
       "26             5.0           3.4            1.6           0.4\n",
       "0              5.1           3.5            1.4           0.2\n",
       "67             5.8           2.7            4.1           1.0\n",
       "57             4.9           2.4            3.3           1.0\n",
       "20             5.4           3.4            1.7           0.2\n",
       "88             5.6           3.0            4.1           1.3"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Build the model with following layers: <br>\n",
    "1. First dense layer with 10 neurons with input shape 4 (according to the feature set) <br>\n",
    "2. Second Dense layer with 8 neurons <br>\n",
    "3. Output layer with 3 neurons with softmax activation (output layer, 3 neurons as we have 3 classes) <br>\n",
    "4. Use SGD and categorical_crossentropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Sequential Graph (model)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2nd hidden layer\n",
    "model.add(tf.keras.layers.Dense(8, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2nd hidden layer\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_v1_8 (Ba (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 181\n",
      "Trainable params: 173\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 105 samples\n",
      "Epoch 1/15\n",
      "105/105 [==============================] - 1s 5ms/sample - loss: 1.1849 - acc: 0.3429 - val_loss: 1.1860 - val_acc: 0.3429\n",
      "Epoch 2/15\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 1.1787 - acc: 0.3429 - val_loss: 1.1826 - val_acc: 0.3429\n",
      "Epoch 3/15\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 1.1746 - acc: 0.3429 - val_loss: 1.1761 - val_acc: 0.3429\n",
      "Epoch 4/15\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 1.1683 - acc: 0.3429 - val_loss: 1.1718 - val_acc: 0.3429\n",
      "Epoch 5/15\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 1.1636 - acc: 0.3429 - val_loss: 1.1671 - val_acc: 0.3429\n",
      "Epoch 6/15\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 1.1590 - acc: 0.3429 - val_loss: 1.1620 - val_acc: 0.3429\n",
      "Epoch 7/15\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 1.1540 - acc: 0.3429 - val_loss: 1.1579 - val_acc: 0.3429\n",
      "Epoch 8/15\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 1.1504 - acc: 0.3429 - val_loss: 1.1539 - val_acc: 0.3429\n",
      "Epoch 9/15\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 1.1462 - acc: 0.3429 - val_loss: 1.1507 - val_acc: 0.3429\n",
      "Epoch 10/15\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 1.1436 - acc: 0.3429 - val_loss: 1.1476 - val_acc: 0.3429\n",
      "Epoch 11/15\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 1.1401 - acc: 0.3429 - val_loss: 1.1437 - val_acc: 0.3429\n",
      "Epoch 12/15\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 1.1364 - acc: 0.3429 - val_loss: 1.1395 - val_acc: 0.3429\n",
      "Epoch 13/15\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 1.1323 - acc: 0.3429 - val_loss: 1.1356 - val_acc: 0.3429\n",
      "Epoch 14/15\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 1.1290 - acc: 0.3429 - val_loss: 1.1325 - val_acc: 0.3429\n",
      "Epoch 15/15\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 1.1254 - acc: 0.3429 - val_loss: 1.1295 - val_acc: 0.3429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x142202e3588>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,          \n",
    "          validation_data=(X_train,y_train),\n",
    "          epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Accuracy of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24053654, 0.39096498, 0.36849847],\n",
       "       [0.23694304, 0.4098933 , 0.35316372],\n",
       "       [0.23860824, 0.40435398, 0.35703778],\n",
       "       [0.23719049, 0.40908042, 0.35372904],\n",
       "       [0.23742896, 0.40849152, 0.3540795 ],\n",
       "       [0.2402622 , 0.38955727, 0.3701805 ],\n",
       "       [0.23829056, 0.40416843, 0.35754102],\n",
       "       [0.23769398, 0.40792373, 0.35438234],\n",
       "       [0.2402923 , 0.38890603, 0.37080166],\n",
       "       [0.23810415, 0.40750876, 0.35438704],\n",
       "       [0.24034443, 0.39002037, 0.3696352 ],\n",
       "       [0.23797832, 0.4084644 , 0.35355726],\n",
       "       [0.23764077, 0.40766266, 0.35469654],\n",
       "       [0.23785812, 0.40821573, 0.35392612],\n",
       "       [0.23627909, 0.41080472, 0.35291615],\n",
       "       [0.24046706, 0.39151856, 0.36801434],\n",
       "       [0.23928061, 0.40241098, 0.3583084 ],\n",
       "       [0.24029419, 0.39206222, 0.36764356],\n",
       "       [0.23878102, 0.40379536, 0.35742357],\n",
       "       [0.23736307, 0.4088721 , 0.35376483],\n",
       "       [0.24034356, 0.38884968, 0.37080678],\n",
       "       [0.23879535, 0.40404105, 0.35716364],\n",
       "       [0.2380642 , 0.4068512 , 0.35508457],\n",
       "       [0.2401367 , 0.38794565, 0.37191767],\n",
       "       [0.23837389, 0.4027875 , 0.35883868],\n",
       "       [0.23842625, 0.4042105 , 0.35736322],\n",
       "       [0.23677297, 0.40876752, 0.35445952],\n",
       "       [0.23803645, 0.40687466, 0.35508886],\n",
       "       [0.2406106 , 0.39216626, 0.36722314],\n",
       "       [0.23802842, 0.40431657, 0.357655  ],\n",
       "       [0.23887837, 0.40271512, 0.35840648],\n",
       "       [0.23699172, 0.4084776 , 0.35453063],\n",
       "       [0.23720023, 0.40972644, 0.3530733 ],\n",
       "       [0.23724298, 0.40818554, 0.35457152],\n",
       "       [0.23770316, 0.40898648, 0.35331035],\n",
       "       [0.2404385 , 0.39285654, 0.366705  ],\n",
       "       [0.23859052, 0.4043561 , 0.3570535 ],\n",
       "       [0.23752011, 0.40946388, 0.35301605],\n",
       "       [0.23727155, 0.41023225, 0.35249618],\n",
       "       [0.24027441, 0.39084682, 0.36887872],\n",
       "       [0.24020953, 0.3879197 , 0.3718708 ],\n",
       "       [0.23886296, 0.40228155, 0.35885555],\n",
       "       [0.23940341, 0.40276003, 0.3578365 ],\n",
       "       [0.23997493, 0.38795108, 0.37207395],\n",
       "       [0.23893099, 0.40396112, 0.3571079 ]], dtype=float32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.109591343667772, 0.31111112]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-8994abc3fa2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \"\"\"\n\u001b[0;32m   1523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "_,_,_=classification_report(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Linear+classification+using+Tensorflow+and+Keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
