{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_External_Lab_Questions-HYD_AIML_Nov18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGIsF1ADyJ58"
      },
      "source": [
        "# Transfer Learning CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-n6tVFayGBe"
      },
      "source": [
        "* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n",
        "* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq8ejXHJyGYq"
      },
      "source": [
        "### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr1O0XzGNtDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "c25e9c20-4497-4b08-a4b2-d223a5329928"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import applications\n",
        "from keras.models import Sequential, Model \n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import vis\n",
        "import keras as keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uWYbxnBayFUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "71da262f-c96a-48a4-b7fe-8a641bd988de"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 12s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLgSnJXN4wD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e2633431-f5cb-49b2-80b3-09967d944892"
      },
      "source": [
        "y_train_new = y_train[:, 0]\n",
        "y_test_new = y_test[:, 0]\n",
        "print(y_train_new.shape)\n",
        "print(y_test_new.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTW_yCCqOC_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create two datasets one with classes from 0 to 4 and one with 5 to 9\n",
        "x_train_lt5 = x_train[y_train_new < 5]\n",
        "y_train_lt5 = y_train_new[y_train_new < 5]\n",
        "x_test_lt5 = x_test[y_test_new < 5]\n",
        "y_test_lt5 = y_test_new[y_test_new < 5]\n",
        "\n",
        "x_train_gt5 = x_train[y_train_new >= 5]\n",
        "y_train_gt5 = y_train_new[y_train_new >= 5]\n",
        "x_test_gt5 = x_test[y_test_new >= 5]\n",
        "y_test_gt5 = y_test_new[y_test_new >= 5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtCKmQh4yXhT"
      },
      "source": [
        "### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uN5O2kJ3yYa6",
        "colab": {}
      },
      "source": [
        "# number of classes\n",
        "num_classes = 10\n",
        "# convert class vectors to binary class matrices\n",
        "train_labels_lt5 = keras.utils.to_categorical(y_train_lt5, 10)\n",
        "test_labels_lt5 = keras.utils.to_categorical(y_test_lt5, 10)\n",
        "#Changing into float and Normalizing the input\n",
        "train_features_lt5 = x_train_lt5.astype('float32')/255\n",
        "test_features_lt5 = x_test_lt5.astype('float32')/255\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cuOiKWfeybAl"
      },
      "source": [
        "### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HzxNbiiyoBD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3b444af7-3e99-4aca-bfe6-7c9e678bf567"
      },
      "source": [
        "# input image dimensions\n",
        "IMG_SIZE = 32\n",
        "filters = 32\n",
        "pool_size = 2\n",
        "# convolution kernel size\n",
        "kernel_size = 3\n",
        "\n",
        "\n",
        "conv_layers = [\n",
        "    Conv2D(filters, kernel_size,\n",
        "           padding='valid',\n",
        "           input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size = pool_size),\n",
        "    Conv2D(64, kernel_size),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size = pool_size),\n",
        "    Conv2D(96, kernel_size),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size = pool_size),\n",
        "    Flatten(),\n",
        "]\n",
        "\n",
        "output_layers = [\n",
        "    Dense(128),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(num_classes),\n",
        "    Activation('softmax')\n",
        "]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fXLpE3rQNyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Attempting to use Data Augmentation\n",
        "data = ImageDataGenerator(samplewise_center=False, # set input mean to 0 over the sample\n",
        "                          samplewise_std_normalization=False,  # divide inputs by std of the sample\n",
        "                          rotation_range=90,       # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                          width_shift_range=0.2,   # randomly shift images horizontally (fraction of total width)\n",
        "                          height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "                          fill_mode='reflect',     # filling the area outside\n",
        "                          zoom_range=0.4,          # random zoom\n",
        "                          horizontal_flip=True,    # randomly flip images\n",
        "                          vertical_flip=True)      # randomly flip images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hstp9-aQd3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "0c2e10a6-1b5d-4946-d0d7-c3fcb33ec1d3"
      },
      "source": [
        "# create complete model\n",
        "model = Sequential(conv_layers + output_layers)\n",
        "# Save the model \n",
        "checkpoint = ModelCheckpoint(\"init_model_upto_4.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
        "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqRdERuuQkEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "3da6750f-a800-4932-ea2e-2deec27989db"
      },
      "source": [
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(data.flow(train_features_lt5, train_labels_lt5, batch_size = 512),\n",
        "          steps_per_epoch = train_features_lt5.shape[0]/350, epochs = 5,\n",
        "          verbose = 1,\n",
        "          callbacks = [checkpoint, early],\n",
        "          validation_data= (test_features_lt5, test_labels_lt5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/5\n",
            "72/71 [==============================] - 101s 1s/step - loss: 0.2311 - acc: 0.9065 - val_loss: 0.2329 - val_acc: 0.8991\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89908, saving model to init_model_upto_4.h5\n",
            "Epoch 2/5\n",
            "72/71 [==============================] - 95s 1s/step - loss: 0.1964 - acc: 0.9151 - val_loss: 0.1767 - val_acc: 0.9239\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.89908 to 0.92394, saving model to init_model_upto_4.h5\n",
            "Epoch 3/5\n",
            "72/71 [==============================] - 99s 1s/step - loss: 0.1847 - acc: 0.9199 - val_loss: 0.1754 - val_acc: 0.9232\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.92394\n",
            "Epoch 4/5\n",
            "72/71 [==============================] - 96s 1s/step - loss: 0.1772 - acc: 0.9233 - val_loss: 0.1742 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.92394 to 0.92514, saving model to init_model_upto_4.h5\n",
            "Epoch 5/5\n",
            "72/71 [==============================] - 97s 1s/step - loss: 0.1733 - acc: 0.9248 - val_loss: 0.1666 - val_acc: 0.9282\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.92514 to 0.92822, saving model to init_model_upto_4.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6KJWYhRQ2M4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "11c9b121-c8fe-43f4-87df-f6d6acef81e3"
      },
      "source": [
        "output_model_train = model.evaluate(train_features_lt5, train_labels_lt5)\n",
        "output_model_test = model.evaluate(test_features_lt5, test_labels_lt5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 23s 912us/step\n",
            "5000/5000 [==============================] - 5s 913us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GogidRvMQ3NR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f3404a30-b474-4aff-cb78-d5034002389e"
      },
      "source": [
        "print('Model Train Accuracy:', output_model_train[1] * 100, \"%\")\n",
        "print('Model Test accuracy:', output_model_test[1] * 100, \"%\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Train Accuracy: 92.89479954528808 %\n",
            "Model Test accuracy: 92.82199912071228 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAEMiDegQ38N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "woTfNst_ynRG"
      },
      "source": [
        "### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN-6NM__Q1Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers:\n",
        "    if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n",
        "    #Freezing a layer\n",
        "        layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gVbCrw-QMwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_VCDB3Byb1a",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1-uUPqWpyeyX"
      },
      "source": [
        "### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n",
        "Achieve an accuracy of more than 85% on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szHjJgDvyfCt",
        "colab": {}
      },
      "source": [
        "# convert class vectors to binary class matrices\n",
        "train_labels_gt5 = keras.utils.to_categorical(y_train_gt5, 10)\n",
        "test_labels_gt5 = keras.utils.to_categorical(y_test_gt5, 10)\n",
        "#Changing into float and Normalizing the input\n",
        "train_features_gt5 = x_train_gt5.astype('float32')/255\n",
        "test_features_gt5 = x_test_gt5.astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQk5v_vhRdiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "84b8f0da-4fe2-4070-f8d3-c8681dc4b0b7"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.convolutional.Conv2D at 0x7f427f53ca90>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f427f53cd30>,\n",
              " <keras.layers.core.Activation at 0x7f427f53ce48>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f427f53ce80>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f427f53cf28>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f427f5a70f0>,\n",
              " <keras.layers.core.Activation at 0x7f427f5a7208>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f427f5a7240>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f427f5a72e8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f427f5a7470>,\n",
              " <keras.layers.core.Activation at 0x7f427f5a7588>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f427f5a75c0>,\n",
              " <keras.layers.core.Flatten at 0x7f427f5a7668>,\n",
              " <keras.layers.core.Dense at 0x7f427f5a7710>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f427f5a7898>,\n",
              " <keras.layers.core.Activation at 0x7f427f5a79b0>,\n",
              " <keras.layers.core.Dropout at 0x7f427f5a79e8>,\n",
              " <keras.layers.core.Dense at 0x7f427f5a7a20>,\n",
              " <keras.layers.core.Activation at 0x7f427f5a7ba8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCL5yI6ZRepW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trans_model = Sequential(model.layers[:13])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ_1KdFSRkNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "1b17566e-7426-4ac5-a591-a3091a0ea493"
      },
      "source": [
        "trans_model.add(Dense(256))\n",
        "trans_model.add(Activation('relu'))\n",
        "trans_model.add(Dropout(0.25))\n",
        "trans_model.add(Dense(10))\n",
        "trans_model.add(Activation('softmax'))\n",
        "trans_model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 96)          55392     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 96)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 384)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               98560     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 176,682\n",
            "Trainable params: 101,130\n",
            "Non-trainable params: 75,552\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzvUhVgiRqnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "2d5a4748-8afb-42ee-a24f-b4ffcc2fc254"
      },
      "source": [
        "trans_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "trans_model.fit(train_features_gt5, train_labels_gt5, batch_size = 256, epochs = 5,verbose = 2,\n",
        "          validation_data= (test_features_gt5, test_labels_gt5))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            " - 21s - loss: 0.7917 - acc: 0.7046 - val_loss: 0.9535 - val_acc: 0.6394\n",
            "Epoch 2/5\n",
            " - 20s - loss: 0.6612 - acc: 0.7537 - val_loss: 1.0637 - val_acc: 0.6314\n",
            "Epoch 3/5\n",
            " - 20s - loss: 0.6137 - acc: 0.7744 - val_loss: 1.0541 - val_acc: 0.6374\n",
            "Epoch 4/5\n",
            " - 20s - loss: 0.5712 - acc: 0.7909 - val_loss: 1.2151 - val_acc: 0.6060\n",
            "Epoch 5/5\n",
            " - 20s - loss: 0.5467 - acc: 0.7986 - val_loss: 1.1067 - val_acc: 0.6328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4272d62b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FU-HwvIdH0M-"
      },
      "source": [
        "## Sentiment analysis <br> \n",
        "\n",
        "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
        "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAQDiZHRH0M_"
      },
      "source": [
        "### 6. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNSs567SNoki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3eXGIe-SH0NA",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./tweets.csv', encoding = \"ISO-8859-1\").dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWeWe1eJH0NF",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7kX-WoJDH0NV",
        "outputId": "52b2bffb-65b6-470c-dc0c-73a550f9b759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
              "      <td>iPhone</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
              "      <td>Google</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n",
              "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n",
              "1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n",
              "2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n",
              "3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n",
              "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGWB3P2WH0NY"
      },
      "source": [
        "### Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdgA_8N2H0NY",
        "colab": {}
      },
      "source": [
        "data = data[(data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Jlu-reIH0Na",
        "outputId": "c47c2b54-b9fc-4952-f074-482da3ccb8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SotCRvkDH0Nf"
      },
      "source": [
        "### 7. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
        "\n",
        "#### Use `vect` as the variable name for initialising CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YcbkY4sgH0Ng",
        "colab": {}
      },
      "source": [
        "# Term Frequency\n",
        "vect = CountVectorizer()\n",
        "tf = vect.fit_transform(data['tweet_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyXtZGr-H0Nl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed98c889-614b-41c0-e3d7-b78dd651e753"
      },
      "source": [
        "tf.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 5648)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z4LUM-XPH0Nn",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIdZYxJtH0Nq",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5pxd5fSHH0Nt"
      },
      "source": [
        "### 8. Find number of different words in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1DQ2LdNH0Nu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "807b17ef-14f8-4903-b5a1-92cac521d527"
      },
      "source": [
        "dir(tf)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abs__',\n",
              " '__add__',\n",
              " '__array_priority__',\n",
              " '__bool__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__div__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__iadd__',\n",
              " '__idiv__',\n",
              " '__imul__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__isub__',\n",
              " '__iter__',\n",
              " '__itruediv__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__matmul__',\n",
              " '__module__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__neg__',\n",
              " '__new__',\n",
              " '__nonzero__',\n",
              " '__pow__',\n",
              " '__radd__',\n",
              " '__rdiv__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__rmatmul__',\n",
              " '__rmul__',\n",
              " '__rsub__',\n",
              " '__rtruediv__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__sub__',\n",
              " '__subclasshook__',\n",
              " '__truediv__',\n",
              " '__weakref__',\n",
              " '_add_dense',\n",
              " '_add_sparse',\n",
              " '_arg_min_or_max',\n",
              " '_arg_min_or_max_axis',\n",
              " '_asindices',\n",
              " '_binopt',\n",
              " '_cs_matrix__get_has_canonical_format',\n",
              " '_cs_matrix__get_sorted',\n",
              " '_cs_matrix__set_has_canonical_format',\n",
              " '_cs_matrix__set_sorted',\n",
              " '_deduped_data',\n",
              " '_divide',\n",
              " '_divide_sparse',\n",
              " '_get_arrayXarray',\n",
              " '_get_arrayXint',\n",
              " '_get_arrayXslice',\n",
              " '_get_columnXarray',\n",
              " '_get_dtype',\n",
              " '_get_intXarray',\n",
              " '_get_intXint',\n",
              " '_get_intXslice',\n",
              " '_get_sliceXarray',\n",
              " '_get_sliceXint',\n",
              " '_get_sliceXslice',\n",
              " '_get_submatrix',\n",
              " '_imag',\n",
              " '_inequality',\n",
              " '_insert_many',\n",
              " '_major_index_fancy',\n",
              " '_major_slice',\n",
              " '_maximum_minimum',\n",
              " '_min_or_max',\n",
              " '_min_or_max_axis',\n",
              " '_minor_index_fancy',\n",
              " '_minor_reduce',\n",
              " '_minor_slice',\n",
              " '_mul_multivector',\n",
              " '_mul_scalar',\n",
              " '_mul_sparse_matrix',\n",
              " '_mul_vector',\n",
              " '_prepare_indices',\n",
              " '_process_toarray_args',\n",
              " '_real',\n",
              " '_rsub_dense',\n",
              " '_scalar_binopt',\n",
              " '_set_arrayXarray',\n",
              " '_set_arrayXarray_sparse',\n",
              " '_set_dtype',\n",
              " '_set_intXint',\n",
              " '_set_many',\n",
              " '_set_self',\n",
              " '_setdiag',\n",
              " '_shape',\n",
              " '_sub_dense',\n",
              " '_sub_sparse',\n",
              " '_swap',\n",
              " '_validate_indices',\n",
              " '_with_data',\n",
              " '_zero_many',\n",
              " 'arcsin',\n",
              " 'arcsinh',\n",
              " 'arctan',\n",
              " 'arctanh',\n",
              " 'argmax',\n",
              " 'argmin',\n",
              " 'asformat',\n",
              " 'asfptype',\n",
              " 'astype',\n",
              " 'ceil',\n",
              " 'check_format',\n",
              " 'conj',\n",
              " 'conjugate',\n",
              " 'copy',\n",
              " 'count_nonzero',\n",
              " 'data',\n",
              " 'deg2rad',\n",
              " 'diagonal',\n",
              " 'dot',\n",
              " 'dtype',\n",
              " 'eliminate_zeros',\n",
              " 'expm1',\n",
              " 'floor',\n",
              " 'format',\n",
              " 'getH',\n",
              " 'get_shape',\n",
              " 'getcol',\n",
              " 'getformat',\n",
              " 'getmaxprint',\n",
              " 'getnnz',\n",
              " 'getrow',\n",
              " 'has_canonical_format',\n",
              " 'has_sorted_indices',\n",
              " 'indices',\n",
              " 'indptr',\n",
              " 'log1p',\n",
              " 'max',\n",
              " 'maximum',\n",
              " 'maxprint',\n",
              " 'mean',\n",
              " 'min',\n",
              " 'minimum',\n",
              " 'multiply',\n",
              " 'ndim',\n",
              " 'nnz',\n",
              " 'nonzero',\n",
              " 'power',\n",
              " 'prune',\n",
              " 'rad2deg',\n",
              " 'reshape',\n",
              " 'resize',\n",
              " 'rint',\n",
              " 'set_shape',\n",
              " 'setdiag',\n",
              " 'shape',\n",
              " 'sign',\n",
              " 'sin',\n",
              " 'sinh',\n",
              " 'sort_indices',\n",
              " 'sorted_indices',\n",
              " 'sqrt',\n",
              " 'sum',\n",
              " 'sum_duplicates',\n",
              " 'tan',\n",
              " 'tanh',\n",
              " 'toarray',\n",
              " 'tobsr',\n",
              " 'tocoo',\n",
              " 'tocsc',\n",
              " 'tocsr',\n",
              " 'todense',\n",
              " 'todia',\n",
              " 'todok',\n",
              " 'tolil',\n",
              " 'transpose',\n",
              " 'trunc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwtgjTBeH0Ny"
      },
      "source": [
        "#### Tip: To see all available functions for an Object use dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2n_iCcTNH0N0",
        "outputId": "7bfc0e76-4e64-4679-87c1-2d9bf941d17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dir(tf)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__abs__',\n",
              " '__add__',\n",
              " '__array_priority__',\n",
              " '__bool__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__div__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__iadd__',\n",
              " '__idiv__',\n",
              " '__imul__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__isub__',\n",
              " '__iter__',\n",
              " '__itruediv__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__matmul__',\n",
              " '__module__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__neg__',\n",
              " '__new__',\n",
              " '__nonzero__',\n",
              " '__pow__',\n",
              " '__radd__',\n",
              " '__rdiv__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__rmatmul__',\n",
              " '__rmul__',\n",
              " '__rsub__',\n",
              " '__rtruediv__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__sub__',\n",
              " '__subclasshook__',\n",
              " '__truediv__',\n",
              " '__weakref__',\n",
              " '_add_dense',\n",
              " '_add_sparse',\n",
              " '_arg_min_or_max',\n",
              " '_arg_min_or_max_axis',\n",
              " '_asindices',\n",
              " '_binopt',\n",
              " '_cs_matrix__get_has_canonical_format',\n",
              " '_cs_matrix__get_sorted',\n",
              " '_cs_matrix__set_has_canonical_format',\n",
              " '_cs_matrix__set_sorted',\n",
              " '_deduped_data',\n",
              " '_divide',\n",
              " '_divide_sparse',\n",
              " '_get_arrayXarray',\n",
              " '_get_arrayXint',\n",
              " '_get_arrayXslice',\n",
              " '_get_columnXarray',\n",
              " '_get_dtype',\n",
              " '_get_intXarray',\n",
              " '_get_intXint',\n",
              " '_get_intXslice',\n",
              " '_get_sliceXarray',\n",
              " '_get_sliceXint',\n",
              " '_get_sliceXslice',\n",
              " '_get_submatrix',\n",
              " '_imag',\n",
              " '_inequality',\n",
              " '_insert_many',\n",
              " '_major_index_fancy',\n",
              " '_major_slice',\n",
              " '_maximum_minimum',\n",
              " '_min_or_max',\n",
              " '_min_or_max_axis',\n",
              " '_minor_index_fancy',\n",
              " '_minor_reduce',\n",
              " '_minor_slice',\n",
              " '_mul_multivector',\n",
              " '_mul_scalar',\n",
              " '_mul_sparse_matrix',\n",
              " '_mul_vector',\n",
              " '_prepare_indices',\n",
              " '_process_toarray_args',\n",
              " '_real',\n",
              " '_rsub_dense',\n",
              " '_scalar_binopt',\n",
              " '_set_arrayXarray',\n",
              " '_set_arrayXarray_sparse',\n",
              " '_set_dtype',\n",
              " '_set_intXint',\n",
              " '_set_many',\n",
              " '_set_self',\n",
              " '_setdiag',\n",
              " '_shape',\n",
              " '_sub_dense',\n",
              " '_sub_sparse',\n",
              " '_swap',\n",
              " '_validate_indices',\n",
              " '_with_data',\n",
              " '_zero_many',\n",
              " 'arcsin',\n",
              " 'arcsinh',\n",
              " 'arctan',\n",
              " 'arctanh',\n",
              " 'argmax',\n",
              " 'argmin',\n",
              " 'asformat',\n",
              " 'asfptype',\n",
              " 'astype',\n",
              " 'ceil',\n",
              " 'check_format',\n",
              " 'conj',\n",
              " 'conjugate',\n",
              " 'copy',\n",
              " 'count_nonzero',\n",
              " 'data',\n",
              " 'deg2rad',\n",
              " 'diagonal',\n",
              " 'dot',\n",
              " 'dtype',\n",
              " 'eliminate_zeros',\n",
              " 'expm1',\n",
              " 'floor',\n",
              " 'format',\n",
              " 'getH',\n",
              " 'get_shape',\n",
              " 'getcol',\n",
              " 'getformat',\n",
              " 'getmaxprint',\n",
              " 'getnnz',\n",
              " 'getrow',\n",
              " 'has_canonical_format',\n",
              " 'has_sorted_indices',\n",
              " 'indices',\n",
              " 'indptr',\n",
              " 'log1p',\n",
              " 'max',\n",
              " 'maximum',\n",
              " 'maxprint',\n",
              " 'mean',\n",
              " 'min',\n",
              " 'minimum',\n",
              " 'multiply',\n",
              " 'ndim',\n",
              " 'nnz',\n",
              " 'nonzero',\n",
              " 'power',\n",
              " 'prune',\n",
              " 'rad2deg',\n",
              " 'reshape',\n",
              " 'resize',\n",
              " 'rint',\n",
              " 'set_shape',\n",
              " 'setdiag',\n",
              " 'shape',\n",
              " 'sign',\n",
              " 'sin',\n",
              " 'sinh',\n",
              " 'sort_indices',\n",
              " 'sorted_indices',\n",
              " 'sqrt',\n",
              " 'sum',\n",
              " 'sum_duplicates',\n",
              " 'tan',\n",
              " 'tanh',\n",
              " 'toarray',\n",
              " 'tobsr',\n",
              " 'tocoo',\n",
              " 'tocsc',\n",
              " 'tocsr',\n",
              " 'todense',\n",
              " 'todia',\n",
              " 'todok',\n",
              " 'tolil',\n",
              " 'transpose',\n",
              " 'trunc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ShA6D8jKH0N5"
      },
      "source": [
        "### Find out how many Positive and Negative emotions are there.\n",
        "\n",
        "Hint: Use value_counts on that column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7LAl5pzH0N6",
        "outputId": "2e7ae487-9c4d-42a1-a0c1-9552cbfafdf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "pd.value_counts(data['is_there_an_emotion_directed_at_a_brand_or_product'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive emotion    2672\n",
              "Negative emotion     519\n",
              "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUvgj0FoH0N9"
      },
      "source": [
        "###  Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'label'\n",
        "\n",
        "Hint: use map on that column and give labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YftKwFv7H0N9",
        "colab": {}
      },
      "source": [
        "data['label'] = data.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1, 'Negative emotion':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ahXwuwNomG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### 9. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNkwrGgEH0OA",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.tweet_text\n",
        "y = data.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTbu0k23TsCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q5nlCuaaH0OD"
      },
      "source": [
        "## 10. **Predicting the sentiment:**\n",
        "\n",
        "\n",
        "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2AbVYssaH0OE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c55fe71-1019-4549-f602-a74e1957a816"
      },
      "source": [
        "# create document-term matrices\n",
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "\n",
        "# use Naive Bayes to predict the star rating\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_dtm, y_train)\n",
        "y_pred_class = nb.predict(X_test_dtm)\n",
        "\n",
        "# calculate accuracy\n",
        "print (metrics.accuracy_score(y_test, y_pred_class))\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8471177944862155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9l3Oz4UUqwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def logistic_reg(vect):\n",
        "    X_train_dtm = vect.fit_transform(X_train)\n",
        "    print('Features: ', X_train_dtm.shape[1])\n",
        "    X_test_dtm = vect.transform(X_test)\n",
        "    logreg = LogisticRegression()\n",
        "    logreg.fit(X_train_dtm, y_train)\n",
        "    y_pred_class = logreg.predict(X_test_dtm)\n",
        "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sw-0B33tH0Ox"
      },
      "source": [
        "## 11. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "okCTOs1TH0Oy",
        "colab": {}
      },
      "source": [
        "def tokenize_test(vect):\n",
        "    x_train_dtm = vect.fit_transform(x_train)\n",
        "    print('Features: ', x_train_dtm.shape[1])\n",
        "    x_test_dtm = vect.transform(x_test)\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(x_train_dtm, y_train)\n",
        "    y_pred_class = nb.predict(x_test_dtm)\n",
        "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxZ8jfPEH0O0"
      },
      "source": [
        "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdCyAN_IH0O0",
        "colab": {}
      },
      "source": [
        "# include 1-grams and 2-grams\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axepytmgH0O4"
      },
      "source": [
        "### 12. Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HToGkq7vH0O4",
        "colab": {}
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1, 2), stop_words= 'english')\n",
        "tokenize_test(vect)\n",
        "logistic_reg(vect)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOIlJRxoH0O7"
      },
      "source": [
        "### 13. Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fUhff-oH0O8",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2KZNWVkH0PA"
      },
      "source": [
        "### 14. Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3v9XD082H0PB",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "We3JK_SRH0PO"
      },
      "source": [
        "### 15. Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUHrfDCyH0PP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}